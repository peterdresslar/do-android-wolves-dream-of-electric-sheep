{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import pandas as pd  # type: ignore\n",
    "import seaborn as sns  # type: ignore\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Fix the import path\n",
    "from model.model import run\n",
    "\n",
    "# Here are the available parameters you can set:\n",
    "# \n",
    "# Core simulation parameters:\n",
    "# - steps: Number of simulation steps to run (default: 250)\n",
    "# - dt: Time step size (default: 0.02)\n",
    "# - s_start: Initial sheep population (default: 100)\n",
    "# - w_start: Initial wolf population (default: 10)\n",
    "# - sheep_max: Maximum sheep capacity (default: 110)\n",
    "# \n",
    "# Wolf behavior parameters:\n",
    "# - no_ai: If True, wolves use fixed theta; if False, wolves use AI (default: False)\n",
    "# - prompt_type: \"high\" or \"low\" (default: \"high\")... amount of information provided to the wolves\n",
    "# - theta_star: Default theta value when no_ai=True (default: 0.5)\n",
    "# - churn_rate: Percentage of wolves that update their theta each step (default: 0.05)\n",
    "# - threads: Number of parallel LLM calls when using async mode (default: 5)\n",
    "#\n",
    "# Wolf population dynamics parameters:\n",
    "# - beta: Predation rate (default: 0.1)\n",
    "# - gamma: Wolf death rate (default: 1.5)\n",
    "# - delta: Conversion efficiency (default: 0.75)\n",
    "#\n",
    "# Sheep population dynamics parameters:\n",
    "# - alpha: Sheep growth rate (default: 1)\n",
    "# - eps: Small value to prevent extinction (default: 0.0001)\n",
    "#\n",
    "# Output parameters:\n",
    "# - save_results: Whether to save simulation results (default: True)\n",
    "# - path: Directory to save results (default: \"../data/results\")\n",
    "\n",
    "\n",
    "\n",
    "# Don ªt set no_ai or prompt_type here, it will be set in the calls below\n",
    "my_args = {\n",
    "    \"steps\": 20,\n",
    "    \"theta_star\": 0.5,\n",
    "    \"churn_rate\": .2,\n",
    "    \"dt\": 0.02,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Information AI Wolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model with AI-enabled wolves\n",
    "ai_results = run(\n",
    "    **my_args,\n",
    "    no_ai=False,\n",
    "    save_results=True,\n",
    "    prompt_type=\"high\"\n",
    ")\n",
    "\n",
    "# Print some information about the results\n",
    "print(f\"Final sheep population: {ai_results['sheep_history'][-1]}\")\n",
    "print(f\"Final wolf population: {ai_results['wolf_history'][-1]}\")\n",
    "print(f\"Thetas: {ai_results['average_theta_history']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Information Prompted AI Wolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model with AI-enabled wolves\n",
    "ai_results_low = run(\n",
    "    **my_args,\n",
    "    no_ai=False,\n",
    "    save_results=True,\n",
    "    prompt_type=\"low\"\n",
    ")\n",
    "\n",
    "# Print some information about the results\n",
    "print(f\"Final sheep population: {ai_results_low['sheep_history'][-1]}\")\n",
    "print(f\"Final wolf population: {ai_results_low['wolf_history'][-1]}\")\n",
    "print(f\"Thetas: {ai_results_low['average_theta_history']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Wolves (no_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model with no_ai=True (note the correct keyword argument syntax)\n",
    "results = run(\n",
    "    **my_args,\n",
    "    no_ai=True,\n",
    "    save_results=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
